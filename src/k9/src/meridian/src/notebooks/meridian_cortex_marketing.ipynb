{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BavGhGX7PcpK"
      },
      "source": [
        "# **Meridian with Cortex for Marketing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDlrE2YGn6hs"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7YxecwGoJrN"
      },
      "source": [
        "Welcome to the Meridian with Cortex for Marketing notebook. This *example* notebook showcases the integration between Cortex for Marketing and Meridian.\n",
        "\n",
        "**❗️IMPORTANT❗️** The default configuration parameters and sample data for Meridian are intended for demo purposes only and should not be deployed for production use. Meridian configuration parameters should be chosen with great care as they will influence the behaviour of the model and results. Please consult Meridian documentation for guidance on how to setup the model configuration for your unique business needs and goals. See [Meridian modeling](https://developers.google.com/meridian/docs/basics/about-the-project). If needed consult with an official [Google Meridian partner](https://developers.google.com/meridian/partners) and/or your Google Ads representative. For any questions or issues related to the Google Cloud Cortex Framework itself, see the [Cortex Framework Support](https://cloud.google.com/cortex/docs/support) page.\n",
        "\n",
        "This notebook is intended for automated execution in Colab Enterprise, but can also be used as a starting point for your own experiments.\n",
        "\n",
        "The overall execution flow of the code in the notebook is:\n",
        "1. Install packages (Meridian) and imports\n",
        "2. Load helper functions for GCS and BigQuery\n",
        "3. Load execution configuration from config file in GCS (`configuration/cortex_meridian_config.json`)\n",
        "4. Load Cortex data from Cortex Data Foundation view in BigQuery\n",
        "5. Configure Meridian model and map Cortex Marketing Cross Media and SAP or Oracle EBS sales data to Meridian data model\n",
        "6. Run Meridian sampling and output summary report to GCS (`reporting` folder)\n",
        "7. Run budget optimizer and output results report to GCS (`reporting` folder)\n",
        "8. Save model to GCS (`models` folder)\n",
        "9. Save CSV results to GCS (`csv` folder)\n",
        "10. Generate overview report and save to GCS (`reporting` folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF985fTsm6fj"
      },
      "source": [
        "### Pre-reqs\n",
        "\n",
        "*   Knowledge of Cortex for Marketing deployed on GCP (see https://cloud.google.com/cortex/docs/overview) with relevant marketing and sales data in BQ\n",
        "*   Access to Cortex for Marketing data in BigQuery\n",
        "*   Knowledge of Meridian (see https://developers.google.com/meridian)\n",
        "\n",
        "### Compatibility\n",
        "\n",
        "This notebook has been tested with Cortex Framework version {{TESTED_CORTEX_VERSION}} and Meridian version {{TESTED_MERIDIAN_VERSION}}."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdFoi6tiSWzv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdPY09SsScC9"
      },
      "outputs": [],
      "source": [
        "# @title Install packages\n",
        "!pip install --upgrade \"google-meridian[colab,and-cuda]=={{TESTED_MERIDIAN_VERSION}}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N79Ut9EQiUF2"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def get_package_version(package_name):\n",
        "  try:\n",
        "    output = subprocess.check_output(['pip', 'show', package_name], text=True)\n",
        "    for line in output.splitlines():\n",
        "      if line.startswith('Version: '):\n",
        "        return line.split(': ')[1]\n",
        "    return None  # Package not found or version not available\n",
        "  except subprocess.CalledProcessError:\n",
        "    return None #pip show command failed.\n",
        "\n",
        "package_name = \"google-meridian\"\n",
        "version = get_package_version(package_name)\n",
        "\n",
        "if version:\n",
        "  print(f\"Meridian version: {version}\")\n",
        "else:\n",
        "  print(f\"Could not find version information for {package_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO4Sizs7Uk6o"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "from pprint import pprint\n",
        "import sys\n",
        "\n",
        "import google.auth\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "import IPython\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import arviz as az\n",
        "\n",
        "import jinja2\n",
        "\n",
        "from meridian import constants\n",
        "from meridian.data import load\n",
        "from meridian.data import test_utils\n",
        "from meridian.model import model\n",
        "from meridian.model import spec\n",
        "from meridian.model import prior_distribution\n",
        "from meridian.analysis import optimizer\n",
        "from meridian.analysis import analyzer\n",
        "from meridian.analysis import visualizer\n",
        "from meridian.analysis import summarizer\n",
        "from meridian.analysis import formatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFu19m3HXt8T"
      },
      "outputs": [],
      "source": [
        "# @title Output system specs\n",
        "\n",
        "# Check RAM, CPUs and if GPU is available\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGzjjxQS_2Cq"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OCcffDlSpZf"
      },
      "outputs": [],
      "source": [
        "# @title GCS helper functions\n",
        "\n",
        "def check_files_in_gcs_folder(bucket_name, folder_path, file_names, project_id):\n",
        "    \"\"\"\n",
        "    Checks if specific files exist within a given folder in a GCS bucket.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "        folder_path: The path to the folder within the bucket (e.g., \"configuration/\").\n",
        "        file_names: A list of file names to check for (e.g., [\"config1.txt\", \"config2.json\"]).\n",
        "        project_id: The ID of the Google Cloud project. If None, the default project will be used.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are file names, and values are booleans indicating\n",
        "        whether the file exists (True) or not (False).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client(project=project_id)\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        file_existence = {}\n",
        "        for file_name in file_names:\n",
        "            blob_path = os.path.join(folder_path, file_name)  # Construct the full blob path\n",
        "            blob = bucket.blob(blob_path)\n",
        "            file_existence[file_name] = blob.exists()\n",
        "\n",
        "        return file_existence\n",
        "\n",
        "    except google.auth.exceptions.DefaultCredentialsError:\n",
        "        print(\"Google Cloud credentials not found.\")\n",
        "        return {}  # Return an empty dictionary to indicate failure\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return {} # Return an empty dictionary to indicate failure\n",
        "\n",
        "\n",
        "\n",
        "def upload_file_to_gcs(bucket_name, file_path, destination_blob_name, project_id):\n",
        "    \"\"\"\n",
        "    Uploads a file to a Google Cloud Storage bucket.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "        file_path: The path to the file to upload.\n",
        "        destination_blob_name: The name of the blob in the bucket where the\n",
        "            file will be uploaded.\n",
        "        project_id: The ID of the Google Cloud project. If None, the default\n",
        "            project will be used.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client(project=project_id)\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "        blob.upload_from_filename(file_path)\n",
        "\n",
        "        print(f\"File {file_path} uploaded to gs://{bucket_name}/{destination_blob_name}\")\n",
        "\n",
        "    except google.auth.exceptions.DefaultCredentialsError:\n",
        "        print(\"Google Cloud credentials not found. Please set up Application Default Credentials.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def load_json_from_gcs(bucket_name, file_path, project_id):\n",
        "    \"\"\"\n",
        "    Loads JSON data from a GCS file into a Python dictionary.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "        file_path: The full path to the JSON file within the bucket (e.g., \"configuration/data.json\").\n",
        "        project_id: The ID of the Google Cloud project. If None, the default project will be used.\n",
        "\n",
        "    Returns:\n",
        "        A Python dictionary containing the JSON data, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client(project=project_id)\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(file_path)\n",
        "\n",
        "        if not blob.exists():\n",
        "            print(f\"File '{file_path}' does not exist in bucket '{bucket_name}'.\")\n",
        "            return None\n",
        "\n",
        "        # Download the JSON file as a string\n",
        "        json_string = blob.download_as_string()\n",
        "\n",
        "        # Parse the JSON string into a Python dictionary\n",
        "        json_data = json.loads(json_string)\n",
        "\n",
        "        return json_data\n",
        "\n",
        "    except google.auth.exceptions.DefaultCredentialsError:\n",
        "        print(\"Google Cloud credentials not found. Please set up Application Default Credentials.\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from file '{file_path}'.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g67hRGu78kI8"
      },
      "source": [
        "### BigQuery helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMA0d7eGFDTK"
      },
      "outputs": [],
      "source": [
        "def check_bigquery_columns(project_id, dataset_id, table_or_view_id, column_names):\n",
        "    \"\"\"\n",
        "    Checks if a list of column names exists in a BigQuery table or view.\n",
        "\n",
        "    Args:\n",
        "        project_id (str): The ID of the BigQuery project.\n",
        "        dataset_id (str): The ID of the BigQuery dataset.\n",
        "        table_or_view_id (str): The ID of the BigQuery table or view.\n",
        "        column_names (list[str]): A list of column names to check for.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists:\n",
        "            - The list of columns that exist in the table/view.\n",
        "            - The list of columns that do not exist in the table/view.\n",
        "\n",
        "    Raises:\n",
        "        google.cloud.exceptions.NotFound: If the table or view does not exist.\n",
        "    \"\"\"\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    table_ref = client.dataset(dataset_id).table(table_or_view_id)\n",
        "\n",
        "    try:\n",
        "        table = client.get_table(table_ref)\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "    existing_columns = [field.name for field in table.schema]\n",
        "    found_columns = []\n",
        "    missing_columns = []\n",
        "\n",
        "    for column_name in column_names:\n",
        "        if column_name in existing_columns:\n",
        "            found_columns.append(column_name)\n",
        "        else:\n",
        "            missing_columns.append(column_name)\n",
        "\n",
        "    return found_columns, missing_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kMWqO5l8WRR"
      },
      "outputs": [],
      "source": [
        "def load_cortex_bigquery_meridian_data(project_id, dataset_id, table_id):\n",
        "    \"\"\"\n",
        "    Loads data from a BigQuery table into a Pandas DataFrame.\n",
        "\n",
        "    Meridian requirement:\n",
        "        Time column values must be formatted in yyyy-mm-dd date format.\n",
        "\n",
        "    Args:\n",
        "        project_id: The ID of the Google Cloud project.\n",
        "        dataset_id: The ID of the BigQuery dataset.\n",
        "        table_id: The ID of the BigQuery table.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame containing the data, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construct the full table ID\n",
        "        table_full_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "        # Initialize the BigQuery client\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Construct the query\n",
        "        query = f\"SELECT * FROM `{table_full_id}` ORDER BY time, geo ASC\"\n",
        "\n",
        "        # Run the query and load the results into a DataFrame\n",
        "        query_job = client.query(query)  # Make an API request.\n",
        "        results = query_job.result().to_dataframe() # Waits for query to finish and converts to a Pandas DataFrame.\n",
        "\n",
        "        print(f\"Loaded data from BigQuery into Pandas DataFrame. Found {len(results)} rows.\")\n",
        "        return results\n",
        "\n",
        "    except google.auth.exceptions.DefaultCredentialsError:\n",
        "        print(\"Google Cloud credentials not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spZOtkq06hs8"
      },
      "source": [
        "## Load configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLCxwDfX8Uh8"
      },
      "outputs": [],
      "source": [
        "# @title Set variables\n",
        "# 1. Project ID Configuration\n",
        "gcs_project_id = os.environ.get('GCLOUD_PROJECT') or os.environ.get('GOOGLE_CLOUD_PROJECT')\n",
        "\n",
        "if gcs_project_id:\n",
        "    print(f\"Current Project ID (from environment): {gcs_project_id}\")\n",
        "else:\n",
        "    print(\"Project ID not found in environment variables.\")\n",
        "    gcs_project_id = \"\"  # Set to empty string to avoid errors later\n",
        "\n",
        "# 2. GCS Bucket and Folder Paths\n",
        "gcs_bucket_name_suffix =  \"{{BUCKET_SUFFIX}}\" # default value is \"cortex-meridian\"\n",
        "gcs_bucket_name = f\"{gcs_project_id}-{gcs_bucket_name_suffix}\"\n",
        "gcs_bucket = f\"gs://{gcs_bucket_name}\"\n",
        "\n",
        "gcs_config_folder_path = \"configuration\"\n",
        "gcs_config_file_name = \"cortex_meridian_config.json\"\n",
        "gcs_config_file_path = os.path.join(gcs_config_folder_path, gcs_config_file_name)\n",
        "\n",
        "gcs_reports_folder_path = \"reporting\"\n",
        "gcs_csv_folder_path = \"csv\"\n",
        "gcs_models_folder_path = \"models\"\n",
        "\n",
        "# 3. Timestamp Generation\n",
        "now = datetime.datetime.now()\n",
        "timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
        "print(f\"Timestamp: {timestamp_str} will be added to all files\")\n",
        "\n",
        "# 4. Local File Paths\n",
        "local_reports_path = '/content/reporting'\n",
        "local_csv_path = '/content/csv'\n",
        "local_models_path = '/content/models'\n",
        "\n",
        "# 5. Filename Definitions\n",
        "optimization_report_filename = f'optimization_output_{timestamp_str}.html'\n",
        "summary_report_filename = f'summary_output_{timestamp_str}.html'\n",
        "overview_report_template_filename = 'cortex_meridian_overview.html.jinja'\n",
        "overview_report_filename = 'overview.html'\n",
        "nonoptimized_data_results_filename = f\"nonoptimized_results_{timestamp_str}.csv\"\n",
        "optimized_data_results_file_name = f\"optimized_results_{timestamp_str}.csv\"\n",
        "model_filename = f\"mmm_{timestamp_str}.pkl\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHRHQyIUwt77"
      },
      "outputs": [],
      "source": [
        "# @title Local folders\n",
        "folder_paths = [local_reports_path, local_csv_path, local_models_path]\n",
        "for folder_path in folder_paths:\n",
        "        try:\n",
        "            if not os.path.exists(folder_path):\n",
        "                os.makedirs(folder_path)\n",
        "                print(f\"Folder '{folder_path}' created successfully.\")\n",
        "            else:\n",
        "                print(f\"Folder '{folder_path}' already exists.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error creating folder '{folder_path}': {e}\")\n",
        "            sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlk7cKnZ6k5D"
      },
      "outputs": [],
      "source": [
        "# @title Validate config file exists and load\n",
        "\n",
        "file_check = check_files_in_gcs_folder(gcs_bucket_name, gcs_config_folder_path, [gcs_config_file_name], gcs_project_id)\n",
        "\n",
        "if file_check[gcs_config_file_name]:\n",
        "    print(\"File exists\")\n",
        "\n",
        "    cortex_meridian_config = load_json_from_gcs(gcs_bucket_name, gcs_config_file_path, gcs_project_id)\n",
        "\n",
        "    if cortex_meridian_config:\n",
        "        print(\"Cortex JSON config data loaded successfully:\")\n",
        "        pprint(cortex_meridian_config)\n",
        "    else:\n",
        "        print(\"Failed to load Cortex JSON config data.\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    print(\"File does not exist\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqGo45taF1GJ"
      },
      "outputs": [],
      "source": [
        "# @title Validate columns from configuration exists in view\n",
        "fields_to_columns = [\n",
        "    cortex_meridian_config[\"column_mappings\"][\"time\"],\n",
        "    cortex_meridian_config[\"column_mappings\"][\"geo\"],\n",
        "    cortex_meridian_config[\"column_mappings\"][\"controls\"],\n",
        "    cortex_meridian_config[\"column_mappings\"][\"population\"],\n",
        "    cortex_meridian_config[\"column_mappings\"][\"kpi\"],\n",
        "    cortex_meridian_config[\"column_mappings\"][\"revenue_per_kpi\"],\n",
        "    cortex_meridian_config[\"column_mappings\"][\"media\"],\n",
        "    cortex_meridian_config[\"column_mappings\"][\"media_spend\"]\n",
        "]\n",
        "\n",
        "#Build final list of columns\n",
        "columns_to_check = []\n",
        "for item in fields_to_columns:\n",
        "    if isinstance(item, list):\n",
        "        columns_to_check.extend([x for x in item if x is not None])  # Extend with non-None elements\n",
        "    elif item is not None and item != \"\":\n",
        "        columns_to_check.append(item)  # Append if not None\n",
        "\n",
        "print(f\"Column names read from config: {columns_to_check} will now check view for matching columns\")\n",
        "\n",
        "try:\n",
        "    found, missing = check_bigquery_columns(cortex_meridian_config[\"cortex_bq_project_id\"], cortex_meridian_config[\"cortex_meridian_marketing_data_set_id\"], cortex_meridian_config[\"cortex_meridian_marketing_view_name\"], columns_to_check)\n",
        "    print(f\"Found columns: {found}\")\n",
        "    print(f\"Missing columns: {missing}\")\n",
        "    if len(missing) > 0:\n",
        "        raise Exception(f\"Missing columns: {missing}\")\n",
        "    else:\n",
        "        print(\"All columns found in view.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkptcwUL9XqZ"
      },
      "source": [
        "## Load data from Cortex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzjEc2Ho7zQp"
      },
      "outputs": [],
      "source": [
        "# @title Load from BigQuery into dataframe\n",
        "project_id = cortex_meridian_config[\"cortex_bq_project_id\"]\n",
        "dataset_id = cortex_meridian_config[\"cortex_meridian_marketing_data_set_id\"]\n",
        "table_id = cortex_meridian_config[\"cortex_meridian_marketing_view_name\"]\n",
        "\n",
        "cortex_df = load_cortex_bigquery_meridian_data(project_id, dataset_id, table_id)\n",
        "cortex_df['time'] = cortex_df['time'].astype(str)\n",
        "\n",
        "print(f\"Data frame size: {len(cortex_df)}\")\n",
        "cortex_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B-LFtNq_Jif"
      },
      "outputs": [],
      "source": [
        "# @title Output dataframe info\n",
        "cortex_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRLbmVbqVa86"
      },
      "source": [
        "## Meridian model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**‼ IMPORTANT ‼**: The Meridian modelling parameters read from `cortex_meridian_config.json` should be chosen with great care! **NEVER** make actual changes to your marketing spend before reading and understanding all details about [how to do Meridian modelling](https://developers.google.com/meridian/docs/basics/about-the-project). If needed consult with an [official Google Meridian partner](https://developers.google.com/meridian/partners) and/or your Google Ads representative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1U82fatgKcs"
      },
      "outputs": [],
      "source": [
        "# @title Columns to channel mapping\n",
        "\n",
        "revenue_per_kpi = cortex_meridian_config[\"column_mappings\"][\"revenue_per_kpi\"]\n",
        "\n",
        "base_columns = {\n",
        "    \"time\": cortex_meridian_config[\"column_mappings\"][\"time\"],\n",
        "    \"geo\": cortex_meridian_config[\"column_mappings\"][\"geo\"],\n",
        "    \"population\": cortex_meridian_config[\"column_mappings\"][\"population\"],\n",
        "    \"controls\": cortex_meridian_config[\"column_mappings\"][\"controls\"],\n",
        "    \"media\": cortex_meridian_config[\"column_mappings\"][\"media\"],\n",
        "    \"media_spend\": cortex_meridian_config[\"column_mappings\"][\"media_spend\"],\n",
        "    \"kpi\": cortex_meridian_config[\"column_mappings\"][\"kpi\"]\n",
        "}\n",
        "\n",
        "if revenue_per_kpi and not len(revenue_per_kpi) == 0:\n",
        "    base_columns[\"revenue_per_kpi\"] = revenue_per_kpi\n",
        "else:\n",
        "  print(\"revenue_per_kpi not set will not be added to coord_to_columns\")\n",
        "\n",
        "coord_to_columns = load.CoordToColumns(**base_columns)\n",
        "\n",
        "# Process media array and spend array\n",
        "correct_media_to_channel = {}\n",
        "correct_media_spend_to_channel = {}\n",
        "channel_index = 0\n",
        "for media_name in cortex_meridian_config[\"column_mappings\"][\"media\"]:\n",
        "    channel_name = cortex_meridian_config[\"channel_names\"][channel_index]\n",
        "    correct_media_to_channel[media_name] = channel_name\n",
        "    channel_index += 1\n",
        "\n",
        "channel_index = 0\n",
        "\n",
        "for media_spend_name in cortex_meridian_config[\"column_mappings\"][\"media_spend\"]:\n",
        "  channel_name = cortex_meridian_config[\"channel_names\"][channel_index]\n",
        "  correct_media_spend_to_channel[media_spend_name] = channel_name\n",
        "  channel_index += 1\n",
        "\n",
        "print(\"Media to channel mapping:\", correct_media_to_channel)\n",
        "print(\"Media to channel spend mapping:\", correct_media_spend_to_channel)\n",
        "print(\"coord_to_columns:\", coord_to_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTx5stjbjo7Y"
      },
      "outputs": [],
      "source": [
        "# @title Load dataframe\n",
        "loader = load.DataFrameDataLoader(\n",
        "    df=cortex_df,\n",
        "    kpi_type=cortex_meridian_config[\"data_processing\"][\"kpi_type\"],\n",
        "    coord_to_columns=coord_to_columns,\n",
        "    media_to_channel=correct_media_to_channel,\n",
        "    media_spend_to_channel=correct_media_spend_to_channel,\n",
        ")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XNDd7HX1qTn"
      },
      "outputs": [],
      "source": [
        "# @title Model specs\n",
        "roi_mu = cortex_meridian_config[\"data_processing\"][\"roi_mu\"] # Mu for ROI prior for each media channel.\n",
        "roi_sigma = cortex_meridian_config[\"data_processing\"][\"roi_sigma\"] # Sigma for ROI prior for each media channel.\n",
        "prior = prior_distribution.PriorDistribution(\n",
        "    roi_m=tfp.distributions.LogNormal(roi_mu, roi_sigma, name=constants.ROI_M)\n",
        ")\n",
        "model_spec = spec.ModelSpec(prior=prior)\n",
        "\n",
        "mmm = model.Meridian(input_data=data, model_spec=model_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOJXk0bKEHno"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQyJbrAlzN_B"
      },
      "outputs": [],
      "source": [
        "# @title Sample prior and posterior\n",
        "mmm.sample_prior(cortex_meridian_config[\"data_processing\"][\"sample\"][\"prior\"])\n",
        "mmm.sample_posterior(\n",
        "    n_chains=cortex_meridian_config[\"data_processing\"][\"sample\"][\"posterior\"][\"n_chains\"],\n",
        "    n_adapt=cortex_meridian_config[\"data_processing\"][\"sample\"][\"posterior\"][\"n_adapt\"],\n",
        "    n_burnin=cortex_meridian_config[\"data_processing\"][\"sample\"][\"posterior\"][\"n_burnin\"],\n",
        "    n_keep=cortex_meridian_config[\"data_processing\"][\"sample\"][\"posterior\"][\"n_keep\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD8gxkVpziat"
      },
      "outputs": [],
      "source": [
        "# @title Model fit\n",
        "model_fit = visualizer.ModelFit(mmm)\n",
        "model_fit.plot_model_fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgFvhvJUz0EZ"
      },
      "outputs": [],
      "source": [
        "# @title Summarizer\n",
        "mmm_summarizer = summarizer.Summarizer(mmm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQy95s0PwNyq"
      },
      "outputs": [],
      "source": [
        "# @title Define start/end for report\n",
        "earliest_date = cortex_df['time'].min()\n",
        "latest_date = cortex_df['time'].max()\n",
        "\n",
        "print(\"Earliest date:\", earliest_date)\n",
        "print(\"Latest date:\", latest_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQbN-j8kz0EZ"
      },
      "outputs": [],
      "source": [
        "# @title Generate summary report\n",
        "start_date = earliest_date\n",
        "end_date = latest_date\n",
        "mmm_summarizer.output_model_results_summary(summary_report_filename, local_reports_path, start_date, end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gmAeNfMwids"
      },
      "outputs": [],
      "source": [
        "# @title Save summary report to GCS\n",
        "upload_file_to_gcs(gcs_bucket_name, f\"{local_reports_path}/{summary_report_filename}\",f\"{gcs_reports_folder_path}/{summary_report_filename}\", gcs_project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jns2CdptryyY"
      },
      "source": [
        "### Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38lhqyLvHf51"
      },
      "outputs": [],
      "source": [
        "# @title Budget optimizer\n",
        "budget_optimizer = optimizer.BudgetOptimizer(mmm)\n",
        "optimization_results = budget_optimizer.optimize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at7V7YEh_zwZ"
      },
      "outputs": [],
      "source": [
        "# @title Export optimization report with optimized spend allocations and ROI\n",
        "optimization_results.output_optimization_summary(optimization_report_filename, local_reports_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUzhNIJ2_ZAu"
      },
      "outputs": [],
      "source": [
        "# @title Save optimization report to GCS\n",
        "upload_file_to_gcs(gcs_bucket_name, f\"{local_reports_path}/{optimization_report_filename}\",f\"{gcs_reports_folder_path}/{optimization_report_filename}\", gcs_project_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWa_EXHPWOLg"
      },
      "outputs": [],
      "source": [
        "# @title Output non-optimized summary\n",
        "optimization_results.nonoptimized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN2r_Y9TkAQ6"
      },
      "outputs": [],
      "source": [
        "# @title Save non-optimized summary as CSV local and upload to GCS\n",
        "\n",
        "# Convert xarray Dataset to Pandas DataFrame for saving\n",
        "dataset_nonoptimized_data_results = optimization_results.nonoptimized_data\n",
        "nonoptimized_data_results_df = dataset_nonoptimized_data_results.to_dataframe()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "nonoptimized_data_results_df.to_csv(f\"{local_csv_path}/{nonoptimized_data_results_filename}\")\n",
        "\n",
        "#Upload GCS\n",
        "upload_file_to_gcs(gcs_bucket_name, f\"{local_csv_path}/{nonoptimized_data_results_filename}\",f\"{gcs_csv_folder_path}/{nonoptimized_data_results_filename}\", gcs_project_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C0hUEZfWb_p"
      },
      "outputs": [],
      "source": [
        "# @title Output optimized summary\n",
        "optimization_results.optimized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKHdsmpymp67"
      },
      "outputs": [],
      "source": [
        "# @title Save optimized summary as CSV local and upload to GCS\n",
        "# Convert xarray Dataset to Pandas DataFrame\n",
        "dataset_optimized_data_results = optimization_results.optimized_data\n",
        "optimized_data_results_df = dataset_optimized_data_results.to_dataframe()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "optimized_data_results_df.to_csv(f\"{local_csv_path}/{optimized_data_results_file_name}\")\n",
        "\n",
        "\n",
        "#Upload GCS\n",
        "upload_file_to_gcs(gcs_bucket_name, f\"{local_csv_path}/{optimized_data_results_file_name}\",f\"{gcs_csv_folder_path}/{optimized_data_results_file_name}\", gcs_project_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfaQQ8-fTw0K"
      },
      "outputs": [],
      "source": [
        "# @title Output model object\n",
        "model.save_mmm(mmm, f\"{local_models_path}/{model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP-0ei_QBShA"
      },
      "outputs": [],
      "source": [
        "# @title Save model object to GCS\n",
        "upload_file_to_gcs(gcs_bucket_name, f\"{local_models_path}/{model_filename}\",f\"{gcs_models_folder_path}/{model_filename}\", gcs_project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glZ-7xG9h_xV"
      },
      "source": [
        "## Create/update overview HTML page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS4aAmEVUArA"
      },
      "outputs": [],
      "source": [
        "# @title Get list of reports and CSV files from GCS\n",
        "from google.cloud import storage\n",
        "from google.auth import default\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def remove_before_slash(filepath):\n",
        "  \"\"\"Removes everything before the last '/' in a filepath.\n",
        "\n",
        "  Args:\n",
        "    filepath: The input filepath string.\n",
        "\n",
        "  Returns:\n",
        "    The filepath string with everything before the last '/' removed,\n",
        "    or the original string if no '/' is found.\n",
        "  \"\"\"\n",
        "  last_slash_index = filepath.rfind('/')\n",
        "  if last_slash_index != -1:\n",
        "    return filepath[last_slash_index + 1:]\n",
        "  else:\n",
        "    return filepath\n",
        "\n",
        "def list_gcs_files(bucket_name, folder_path, includeFileExtension):\n",
        "    \"\"\"\n",
        "    Lists files in a Google Cloud Storage (GCS) bucket/folder, sorted by filename descending,\n",
        "    and returns their name, creation date, and authenticated URL.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The name of the GCS bucket.\n",
        "        folder_path (str, optional): The path to the folder within the bucket. Defaults to None (root).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a file and contains\n",
        "              'name', 'created', and 'authenticated_url' keys. Returns an empty list if no files are found.\n",
        "              Returns None if there are errors with GCS.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        credentials, project = default()\n",
        "        storage_client = storage.Client(credentials=credentials, project=project)\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        blobs = bucket.list_blobs(prefix=folder_path) if folder_path else bucket.list_blobs()\n",
        "\n",
        "        file_list = []\n",
        "        for blob in blobs:\n",
        "            if not blob.name.endswith('/'): # Skip folders\n",
        "              if blob.name.endswith(f'{includeFileExtension}'): # Apply file extension filter\n",
        "                file_list.append({\n",
        "                    'name': remove_before_slash(blob.name),\n",
        "                    'created': blob.time_created,\n",
        "                    'authenticated_url': f\"https://storage.mtls.cloud.google.com/{project_id}-{gcs_bucket_name_suffix}/{blob.name}\",\n",
        "                })\n",
        "\n",
        "        # Sort by filename in descending order\n",
        "        sorted_files = sorted(file_list, key=lambda x: x['name'], reverse=True)\n",
        "\n",
        "        return sorted_files\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing GCS: {e}\")\n",
        "        return None\n",
        "\n",
        "def print_files_info(files):\n",
        "    \"\"\"\n",
        "    Prints information about a list of files.\n",
        "\n",
        "    Args:\n",
        "        files: A list of dictionaries, where each dictionary represents a file\n",
        "               and contains 'name', 'created', and 'authenticated_url' keys.\n",
        "               Can also be None.\n",
        "    \"\"\"\n",
        "    if files is not None:\n",
        "        if files:\n",
        "            for file_info in files:\n",
        "                print(f\"Name: {file_info['name']}\")\n",
        "                print(f\"Created: {file_info['created']}\")\n",
        "                print(f\"Authenticated URL: {file_info['authenticated_url']}\")\n",
        "                print(\"-\" * 20)\n",
        "        else:\n",
        "            print(\"No files found.\")\n",
        "\n",
        "print(\"Loading all files on GCS to update overview HTML page\")\n",
        "report_files = list_gcs_files(gcs_bucket_name, f\"{gcs_reports_folder_path}\", \".html\")\n",
        "csv_files = list_gcs_files(gcs_bucket_name, f\"{gcs_csv_folder_path}\", \".csv\")\n",
        "\n",
        "print_files_info(report_files)\n",
        "print_files_info(csv_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRlsWTUvieb9"
      },
      "outputs": [],
      "source": [
        "# @title Load template from GCS to current directory\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "def download_blob(bucket_name, blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The ID of your GCS object\n",
        "    # blob_name = \"storage-object-name\"\n",
        "\n",
        "    # The path to which the file should be downloaded\n",
        "    # destination_file_name = \"/path/to/your/local/file\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Construct a client side representation of a blob.\n",
        "    blob = bucket.blob(blob_name)\n",
        "    try:\n",
        "        blob.download_to_filename(destination_file_name)\n",
        "\n",
        "        print(\n",
        "            \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
        "                blob_name, bucket_name, destination_file_name\n",
        "            )\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {blob_name}: {e}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "download_blob(gcs_bucket_name, f\"{gcs_reports_folder_path}/{overview_report_template_filename}\", f\"/content/{overview_report_template_filename}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKxinrklTJHs"
      },
      "outputs": [],
      "source": [
        "# @title Execute Jinja on template\n",
        "from jinja2 import Environment, FileSystemLoader\n",
        "import datetime\n",
        "\n",
        "# Set up the Jinja2 environment\n",
        "env = Environment(loader=FileSystemLoader('/content'))  # Load templates from the content directory root\n",
        "template = env.get_template(f\"{overview_report_template_filename}\")\n",
        "\n",
        "#Set last update\n",
        "now = datetime.datetime.now()\n",
        "last_update = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Render the template with the data\n",
        "output = template.render(report_files=report_files, csv_files=csv_files, last_update=last_update)\n",
        "\n",
        "# Print the output\n",
        "print(output)\n",
        "\n",
        "# To save to a file:\n",
        "with open(overview_report_filename, 'w') as f:\n",
        "    f.write(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsbQLhH7DSg5"
      },
      "outputs": [],
      "source": [
        "# @title View overview page in notebook (commented out for normal executions)\n",
        "#from IPython.display import HTML\n",
        "#HTML(filename=f\"/content/{overview_report_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCV1r_dzkbgr"
      },
      "outputs": [],
      "source": [
        "# @title Save overview page to GCS\n",
        "upload_file_to_gcs(gcs_bucket_name, f\"{overview_report_filename}\",f\"{gcs_reports_folder_path}/{overview_report_filename}\", gcs_project_id)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "meridian_cortex_marketing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
